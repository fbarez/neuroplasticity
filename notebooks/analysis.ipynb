{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Redistribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from src import BASE_CONCEPT_PATH, PROCESSED_4, NEURONS_PER_LAYER, NUM_LAYERS\n",
    "from src.visualization.results_utils import build_heatmap_data, filter_ablated\n",
    "\n",
    "base_df = pd.read_csv(BASE_CONCEPT_PATH)\n",
    "retrain_4_df = pd.read_csv(PROCESSED_4)\n",
    "base_saliency_df = build_heatmap_data(base_df, NEURONS_PER_LAYER * NUM_LAYERS)\n",
    "filtered_retrained_4_df = filter_ablated(retrain_4_df[['neuron-id','current_concepts']])\n",
    "print(len(filtered_retrained_4_df))\n",
    "retrained_4_saliency_df = build_heatmap_data(filtered_retrained_4_df, len(filtered_retrained_4_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of neurons that show concept saliency > 0.5 after pruning and retraining\n",
    "base_count_per_row = (base_saliency_df > 0.5).sum(axis=1)\n",
    "retrained_count_per_row = (retrained_4_saliency_df > 0.5).sum(axis=1)\n",
    "# Normalize with total number of neurons that were not ablated\n",
    "base_saliency_pc = base_count_per_row / base_count_per_row.sum() * 100\n",
    "retrained_saliency_pc = retrained_count_per_row/ retrained_count_per_row.sum() * 100\n",
    "saliency_pc = pd.concat([base_saliency_pc, retrained_saliency_pc], axis=1).round(3)\n",
    "saliency_pc.rename(columns={0: \"Base\", 1: \"Retrained\"}, inplace=True)\n",
    "saliency_pc"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Correlation between similarity vs saliency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from src import PROCESSED_4, SIMILARITY_4, NEURONS_PER_LAYER, NUM_LAYERS\n",
    "from src.visualization.results_utils import build_heatmap_data, build_sim_data\n",
    "\n",
    "retrain_4_df = pd.read_csv(PROCESSED_4)\n",
    "retrain_4_set_df = retrain_4_df[['neuron-id','current_concepts']]\n",
    "retrain_4_heatdf = build_heatmap_data(retrain_4_set_df, NEURONS_PER_LAYER * NUM_LAYERS)\n",
    "\n",
    "retrain_4_sim_df = pd.read_csv(SIMILARITY_4)\n",
    "retrain_4_sim_heatdf = build_sim_data((retrain_4_sim_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from src import NEURONS_PER_LAYER\n",
    "\n",
    "df = pd.DataFrame(columns=['saliency', 'similarity'])\n",
    "count = 0\n",
    "for index, row in retrain_4_heatdf.iterrows():\n",
    "    for column in range(NEURONS_PER_LAYER):\n",
    "        concept_saliency = row[column]\n",
    "        similarity = retrain_4_sim_heatdf.iloc[index][column]\n",
    "        if not math.isnan(concept_saliency) and not math.isnan(similarity):\n",
    "            df.loc[count] = [concept_saliency, similarity]\n",
    "            count += 1\n",
    "   \n",
    "df.to_csv(\"data/processed/regression.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.scatter(df['similarity'], df['saliency'])\n",
    "plt.ylabel('Saliency score')\n",
    "plt.xlabel('Similarity score')\n",
    "plt.title('Concept similarity versus saliency, post-remapping for location names')\n",
    "# Get the current axes object\n",
    "ax = plt.gca()\n",
    "# Set the linewidth of the axes spines\n",
    "ax.spines['top'].set_linewidth(2)     # Top border\n",
    "ax.spines['bottom'].set_linewidth(2)  # Bottom border\n",
    "ax.spines['left'].set_linewidth(2)    # Left border\n",
    "ax.spines['right'].set_linewidth(2)   # Right border\n",
    "\n",
    "plt.savefig('similarity-vs-saliency.pdf')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mean saliency and similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats as st\n",
    "import numpy as np\n",
    "\n",
    "print(\"Saliency\")\n",
    "print(f\"Mean: {df['saliency'].mean()}\")\n",
    "interval = st.norm.interval(confidence=0.95, loc=np.mean(df['saliency']), scale=st.sem(df['saliency']))\n",
    "print(f\"Confidence interval: {interval}\")\n",
    "print(f\"Standard deviation: {df['saliency'].std()}\")\n",
    "interval = st.norm.interval(confidence=0.95, loc=np.std(df['saliency']), scale=st.sem(df['saliency']))\n",
    "print(f\"Confidence interval: {interval}\")\n",
    "\n",
    "print(\"Similarity\")\n",
    "print(f\"Mean: {df['similarity'].mean()}\")\n",
    "interval = st.norm.interval(confidence=0.95, loc=np.mean(df['similarity']), scale=st.sem(df['similarity']))\n",
    "print(f\"Confidence interval: {interval}\")\n",
    "print(f\"Standard deviation: {df['similarity'].std()}\")\n",
    "interval = st.norm.interval(confidence=0.95, loc=np.std(df['similarity']), scale=st.sem(df['similarity']))\n",
    "print(f\"Confidence interval: {interval}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Concept saliency over retraining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.stats as st\n",
    "import math\n",
    "from src import BASE_CONCEPT_PATH, NEURONS_PER_LAYER, NUM_LAYERS\n",
    "\n",
    "base_df = pd.read_csv(BASE_CONCEPT_PATH)\n",
    "base_set_df = base_df[['neuron-id','current_concepts']]\n",
    "base_heatdf = build_heatmap_data(base_set_df, NEURONS_PER_LAYER * NUM_LAYERS)\n",
    "\n",
    "# Calculate mean for base model\n",
    "\n",
    "df = pd.DataFrame(columns=['layer', 'mean saliency', 'std', 'error'])\n",
    "for index, row in base_heatdf.iterrows():\n",
    "    row_clean = [0 if math.isnan(x) else x for x in row.tolist()]\n",
    "    average = np.mean(row_clean)\n",
    "    std = np.std(row_clean)\n",
    "    (low, high) = st.norm.interval(confidence=0.95, loc=np.mean(row_clean), scale=st.sem(row_clean))\n",
    "    error = abs(high - low) / 2\n",
    "    df.loc[len(df)] = [len(df), average, std, error]\n",
    "mean_saliency_base = df\n",
    "mean_saliency_base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src import PROCESSED_1, PROCESSED_2, PROCESSED_3, PROCESSED_4, NUM_LAYERS, NEURONS_PER_LAYER\n",
    "from src.visualization.results_utils import build_heatmap_data, mean_saliency\n",
    "\n",
    "retrain_1_df = pd.read_csv(PROCESSED_1)\n",
    "retrain_1_set_df = retrain_1_df[['neuron-id','current_concepts']]\n",
    "retrain_1_heatdf = build_heatmap_data(retrain_1_set_df, NEURONS_PER_LAYER * NUM_LAYERS)\n",
    "mean_saliency_1 = mean_saliency(retrain_1_heatdf)\n",
    "\n",
    "retrain_2_df = pd.read_csv(PROCESSED_2)\n",
    "retrain_2_set_df = retrain_2_df[['neuron-id','current_concepts']]\n",
    "retrain_2_heatdf = build_heatmap_data(retrain_2_set_df, NEURONS_PER_LAYER * NUM_LAYERS)\n",
    "mean_saliency_2 = mean_saliency(retrain_2_heatdf)\n",
    "\n",
    "retrain_3_df = pd.read_csv(PROCESSED_3)\n",
    "retrain_3_set_df = retrain_3_df[['neuron-id','current_concepts']]\n",
    "retrain_3_heatdf = build_heatmap_data(retrain_3_set_df, NEURONS_PER_LAYER * NUM_LAYERS)\n",
    "mean_saliency_3 = mean_saliency(retrain_3_heatdf)\n",
    "\n",
    "retrain_4_df = pd.read_csv(PROCESSED_4)\n",
    "retrain_4_set_df = retrain_4_df[['neuron-id','current_concepts']]\n",
    "retrain_4_heatdf = build_heatmap_data(retrain_4_set_df, NEURONS_PER_LAYER * NUM_LAYERS)\n",
    "mean_saliency_4 = mean_saliency(retrain_4_heatdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_table = mean_saliency_base[['layer', 'mean saliency', 'error']].merge(mean_saliency_1, on='layer', suffixes=('', '_1'))\n",
    "merged_table = merged_table.merge(mean_saliency_2, on='layer', suffixes=('', '_2'))\n",
    "merged_table = merged_table.merge(mean_saliency_3, on='layer', suffixes=('', '_3'))\n",
    "merged_table = merged_table.merge(mean_saliency_4, on='layer', suffixes=('', '_4'))\n",
    "merged_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create a new figure and axis\n",
    "fig, ax = plt.subplots(figsize=(6,6))\n",
    "\n",
    "# Iterate over each row in the dataframe\n",
    "time_periods = np.arange(0, 5)\n",
    "ax.set_xticks(time_periods)\n",
    "ax.set_xticklabels([\"Base\", \"2 epochs\", \"4 epochs\", \"6 epochs\", \"8 epochs\"])\n",
    "\n",
    "for _, row in merged_table.iterrows():\n",
    "    layer = row['layer']\n",
    "    mean_base = row['mean saliency']\n",
    "    std_base = row['error']\n",
    "    mean_1 = row['mean_saliency']\n",
    "    std_1 = row['error_1']\n",
    "    mean_2 = row['mean_saliency_2']\n",
    "    std_2 = row['error_2']\n",
    "    mean_3 = row['mean_saliency_3']\n",
    "    std_3 = row['error_3']\n",
    "    mean_4 = row['mean_saliency_4']\n",
    "    std_4 = row['error_4']\n",
    "\n",
    "    line_points = [mean_base, mean_1, mean_2, mean_3, mean_4]\n",
    "    # Plot the line\n",
    "    layer_label = f\"Layer {round(layer)}\"\n",
    "    ax.plot(time_periods, line_points, marker='o', label=layer_label)\n",
    "\n",
    "    # Plot the error bars\n",
    "    ax.errorbar(time_periods, line_points, yerr=[std_base, std_1, std_2, std_3, std_4], color='black', linestyle='None', capsize=4, alpha=0.5)\n",
    "\n",
    "plt.legend()  \n",
    "\n",
    "# Set labels and title\n",
    "ax.set_xlabel('Stage of retraining')\n",
    "ax.set_ylabel('Mean concept saliency')\n",
    "# ax.set_title('Mean saliency in model layers over retraining')\n",
    "\n",
    "# Set the linewidth of the axes spines\n",
    "ax.spines['top'].set_linewidth(2)     # Top border\n",
    "ax.spines['bottom'].set_linewidth(2)  # Bottom border\n",
    "ax.spines['left'].set_linewidth(2)    # Left border\n",
    "ax.spines['right'].set_linewidth(2)   # Right border\n",
    "\n",
    "# Show the plot\n",
    "plt.savefig(\"saliency_over_retraining.pdf\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Concept similarity over retraining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src import SIMILARITY_1, SIMILARITY_2, SIMILARITY_3, SIMILARITY_4\n",
    "\n",
    "retrain_1_sim_df = pd.read_csv(SIMILARITY_1)\n",
    "retrain_1_sim_heatdf = build_sim_data(retrain_1_sim_df)\n",
    "retrain_2_sim_df = pd.read_csv(SIMILARITY_2)\n",
    "retrain_2_sim_heatdf = build_sim_data(retrain_2_sim_df)\n",
    "retrain_3_sim_df = pd.read_csv(SIMILARITY_3)\n",
    "retrain_3_sim_heatdf = build_sim_data(retrain_3_sim_df)\n",
    "retrain_4_sim_df = pd.read_csv(SIMILARITY_4)\n",
    "retrain_4_sim_heatdf = build_sim_data(retrain_4_sim_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.visualization.results_utils import mean_similarity\n",
    "\n",
    "mean_sim_1 = mean_similarity(retrain_1_sim_heatdf)\n",
    "mean_sim_2 = mean_similarity(retrain_2_sim_heatdf)\n",
    "mean_sim_3 = mean_similarity(retrain_3_sim_heatdf)\n",
    "mean_sim_4 = mean_similarity(retrain_4_sim_heatdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_table = mean_sim_1[['layer', 'mean_similarity', 'error']].merge(mean_sim_2, on='layer', suffixes=('', '_2'))\n",
    "merged_table = merged_table.merge(mean_sim_3, on='layer', suffixes=('', '_3'))\n",
    "merged_table = merged_table.merge(mean_sim_4, on='layer', suffixes=('', '_4'))\n",
    "merged_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create a new figure and axis\n",
    "fig, ax = plt.subplots(figsize=(4,4))\n",
    "\n",
    "# Iterate over each row in the dataframe\n",
    "time_periods = np.arange(1, 5)\n",
    "ax.set_xticks(time_periods)\n",
    "ax.set_xticklabels([\"2 epochs\", \"4 epochs\", \"6 epochs\", \"8 epochs\"])\n",
    "\n",
    "for _, row in merged_table.iterrows():\n",
    "    layer = row['layer']\n",
    "    mean_1 = row['mean_similarity']\n",
    "    std_1 = row['error']\n",
    "    mean_2 = row['mean_similarity_2']\n",
    "    std_2 = row['error_2']\n",
    "    mean_3 = row['mean_similarity_3']\n",
    "    std_3 = row['error_3']\n",
    "    mean_4 = row['mean_similarity_4']\n",
    "    std_4 = row['error_4']\n",
    "\n",
    "    line_points = [mean_1, mean_2, mean_3, mean_4]\n",
    "    # Plot the line\n",
    "    layer_label = f\"Layer {round(layer)}\"\n",
    "    ax.plot(time_periods, line_points, marker='o', label=layer_label)\n",
    "\n",
    "    # Plot the error bars\n",
    "    ax.errorbar(time_periods, line_points, yerr=[std_1, std_2, std_3, std_4], color='black', linestyle='None', capsize=4, alpha=0.5)\n",
    "\n",
    "legend = plt.legend(loc='upper center', bbox_to_anchor=(0.5, 1.25), ncol=len(df) // 2)\n",
    "legend.get_frame().set_linewidth(0) \n",
    "\n",
    "# Set labels and title\n",
    "ax.set_xlabel('Stage of retraining')\n",
    "ax.set_ylabel('Mean concept similarity')\n",
    "# ax.set_title('Mean saliency in model layers over retraining')\n",
    "\n",
    "# Set the linewidth of the axes spines\n",
    "ax.spines['top'].set_linewidth(2)     # Top border\n",
    "ax.spines['bottom'].set_linewidth(2)  # Bottom border\n",
    "ax.spines['left'].set_linewidth(2)    # Left border\n",
    "ax.spines['right'].set_linewidth(2)   # Right border\n",
    "\n",
    "# Show the plot\n",
    "plt.savefig(\"similarity_over_retraining.pdf\", bbox_inches='tight')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Examine random HATs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src import PROCESSED_4, NEURONS_PER_LAYER, NUM_LAYERS\n",
    "import ast\n",
    "\n",
    "retrain_4_df = pd.read_csv(PROCESSED_4)\n",
    "df = retrain_4_df[['neuron-id','current_concepts']]\n",
    "retrain_4_heatdf = build_heatmap_data(df, NEURONS_PER_LAYER * NUM_LAYERS)\n",
    "\n",
    "new_df = pd.DataFrame(columns=['neuron-id', 'HAT'])\n",
    "for index, row in df.iterrows():\n",
    "    hat = [word for word, _ in ast.literal_eval(row['current_concepts'])][0]\n",
    "    if hat.isalpha():\n",
    "        new_df.at[index, \"neuron-id\"] = row['neuron-id']\n",
    "        new_df.at[index, \"HAT\"] = hat\n",
    "\n",
    "new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.visualization.results_utils import get_random_hats\n",
    "# Get random HATs from each layer\n",
    "\n",
    "print(get_random_hats(new_df, retrain_4_heatdf, 1))\n",
    "print(get_random_hats(new_df, retrain_4_heatdf, 2))\n",
    "print(get_random_hats(new_df, retrain_4_heatdf, 3))\n",
    "print(get_random_hats(new_df, retrain_4_heatdf, 4))\n",
    "print(get_random_hats(new_df, retrain_4_heatdf, 5))\n",
    "print(get_random_hats(new_df, retrain_4_heatdf, 6))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
